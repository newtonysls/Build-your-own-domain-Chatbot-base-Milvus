# Build-your-own-domain-RAG-base-Milvus
如果基于milvus构建垂域RAG；垂域RAG可以做哪些优化；垂域RAG优化流程

# 使用milvus框架构建垂域RAG应用

## 背景
> 简要介绍RAG的使用背景，熟悉这部分内容的可以跳过

RAG（Retrieval Augmentation Generation），中文叫检索增强生成。RAG的详细介绍会在后续的内容。

### 为什么需要RAG？
从ChatGPT问世以来，LLM百花齐放，通用模型已经具备丰富的背景知识（Trillion级别的语料的预训练和微调）以及数十万、百万级别的输入序列长度支持。那么我们为什么还需要RAG呢？无非以下几点：

1. 模型知识库有限\
LLM的训练时间跨度是非常长的，训练通常花费非常大的计算资源和计算时间。在训练启动前，就需要准备好相关的语料（清洗、去重、数据配比、Instruction数据构建、Reward数据等等），所以模型通过训练学习到的知识无法做到实时的更新。因此诸如垂域文档（产品手册、公司制度手册、作家新写的小说等等），考虑到实时性和保密性，对于任何一个模型内部而言这些知识都是不具备的，并且对通用模型做二次训练也是不现实的。
2. 超长的序列长度造成的时延消耗和性能损失\
首先，由于LLM基于transformers的attention机制，越长的序列长度会带来更多的计算量。尽管有一些attention机制的优化，例如GQA，但是也是有非常大的计算量。因此推理长序列具有非常大的时间消耗。
同时根据scaling laws定律，只有更大参数的LLM才能实现更好的长序列理解性能，进一步的造成了时间的消耗。\
众多的LLM宣传自己模型具有非常好的长序列理解能力，同时也拿出了很多探针实验（大海捞针）做出证明，但是除了时延之外，长序列理解还有很长的路要走。
“探针实验有一定bug。其实相当于测试模型的查询功能。可以理解为训练SPLADE稀疏向量，attention只需要学习到位于不同位置的相同的词语的权重即可，但是距离真正的长序列理解还有一定距离。”

RAG能够构建灵活的知识库，这个知识库是可以编辑、可动态更新的，属于模型的外部知识。这部分知识可以进行灵活的维护，因此RAG能够非常好地解决了第一个问题。部分情况下，回答问题往往并不需要全部的文档，只需要特定相关的文档即可。如果把全部的文档作为知识，那么长序列中的其他无关干扰信息，无疑会对LLM回答进行影响。因此RAG能够一定程度上缓解长序列的问题，其中时延问题也非常好的结果。对于效果部分，RAG的召回文档可能不够全面（召回失败、抽象和概要性问题等等），但是这部分问题从原来针对LLM模型优化（跨度长、成本高）的问题变成优化检索模型和LLM。成本大大降低，优化方式也更加灵活。

综上，目前RAG能够解决非常多的问题，RAG的风靡也促就了非常多的框架对RAG的支持，例如langchain、ollama、milvus等等。对于新学习RAG的人来说，是一个非常好的消息，学习成本大大降低。部分框架只需要几十行代码即可完成简易RAG系统的构建。


### 为什么需要垂域RAG？